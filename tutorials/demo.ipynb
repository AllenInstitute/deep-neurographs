{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a774aa-6f47-4184-b582-bef0033a39ed",
   "metadata": {},
   "source": [
    "# Single Neuron Reconstruction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3a851a2-5ed1-431b-b0f7-dab38b4d3e7b",
   "metadata": {},
   "source": [
    "The first step is to download the following github repo:\n",
    "https://github.com/AllenInstitute/deep-neurographs.git\n",
    "\n",
    "Note that at some point this repo will be renamed \"graph-trace\"."
   ]
  },
  {
   "cell_type": "raw",
   "id": "30d055df-97ca-4165-93f7-6201b64faddf",
   "metadata": {},
   "source": [
    "Now go ahead and run \"pip install -e .\" in the main directory of this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e08838-6fd1-47b4-93b6-cde41e69a7b7",
   "metadata": {},
   "source": [
    "## Part 1: Overview of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8689871b-3280-4c84-81fd-aafba1b913eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from deep_neurographs import evaluation as evaluator, inference, visualization as viz\n",
    "from deep_neurographs.utils import ml_util, util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f663a8f-557c-48d9-a98c-4c20a2979465",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a92db87a-5797-4bfb-858a-64869b36421f",
   "metadata": {},
   "source": [
    "We’ll begin by loading an input graph which is an instance of a class called \"FragmentsGraph\". The connected components (i.e. fragments) in this graph correspond to skeletonized segments from an image segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03b4b4c-a252-4285-a3b4-17caec8969d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deep_neurographs.fragments_graph.FragmentsGraph'>\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "brain_id = \"706301\"\n",
    "example_id = \"000\"\n",
    "plot_bool = False\n",
    "\n",
    "# Read graph\n",
    "input_filename = f\"input_graph_{brain_id}-{example_id}.pkl\"\n",
    "path = f\"./input_graphs/{input_filename}\"\n",
    "with open(path, \"rb\") as file:\n",
    "    input_graph = pickle.load(file)\n",
    "print(type(input_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868873ff-6cdc-4186-a0e7-585553a8194a",
   "metadata": {},
   "source": [
    "### Graph Structure and Attributes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0b1fa80-fc63-4b59-ae29-ac4798a4fab1",
   "metadata": {},
   "source": [
    "A \"FragmentsGraph\" instance is a sparse graph where each node corresponds to either a leaf or branching node (i.e., node with degree 1 or degree ≥ 3). This graph is also geometric in the sense that each node has an \"xyz\" attribute representing a 3D coordinate. The edges contain an array of xyz coordinates which represent the neuron arbor between two points. There are additional attributes including the neuron radius (in microns), fragment filename (swc_id), and proposals.\n",
    "\n",
    "Note that this class is a subclass of networkx.Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8541a17a-1714-4db8-9549-c7d79a122135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node attributes: {'proposals': set(), 'radius': 2.992, 'swc_id': '8739052156', 'xyz': array([20115.559, 15391.285, 24318.135], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "node = util.sample_once(input_graph.nodes)\n",
    "print(\"Node attributes:\", input_graph.nodes[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d851bb-ac29-4ec2-b907-9f342aee2f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge attributes: {'length': 64.49980211257935, 'radius': array([0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 1.249,\n",
      "       1.456, 1.058, 0.748, 1.   , 0.748, 0.748, 0.748, 0.748, 1.249,\n",
      "       1.496, 1.673, 2.115, 2.115, 2.34 , 2.498, 1.948, 1.948, 1.8  ,\n",
      "       1.948, 1.948, 1.496, 1.496, 1.673, 1.8  , 1.249, 1.   , 1.   ,\n",
      "       1.058, 1.496, 1.496, 1.948, 2.115, 1.673, 1.673, 1.673, 1.673,\n",
      "       2.365, 2.244, 2.365, 2.262, 1.948, 2.244, 2.244, 1.673, 1.249,\n",
      "       1.   ], dtype=float16), 'xyz': array([[20338.479, 14864.654, 24037.377],\n",
      "       [20337.451, 14864.604, 24037.723],\n",
      "       [20336.484, 14864.55 , 24038.117],\n",
      "       [20335.574, 14864.493, 24038.56 ],\n",
      "       [20334.723, 14864.434, 24039.053],\n",
      "       [20333.932, 14864.372, 24039.592],\n",
      "       [20333.197, 14864.307, 24040.182],\n",
      "       [20332.523, 14864.238, 24040.818],\n",
      "       [20331.908, 14864.167, 24041.504],\n",
      "       [20331.352, 14864.093, 24042.24 ],\n",
      "       [20330.852, 14864.016, 24043.023],\n",
      "       [20330.412, 14863.937, 24043.854],\n",
      "       [20330.033, 14863.854, 24044.734],\n",
      "       [20329.71 , 14863.768, 24045.664],\n",
      "       [20329.447, 14863.68 , 24046.643],\n",
      "       [20329.2  , 14863.588, 24047.629],\n",
      "       [20328.918, 14863.493, 24048.584],\n",
      "       [20328.61 , 14863.396, 24049.508],\n",
      "       [20328.268, 14863.296, 24050.4  ],\n",
      "       [20327.896, 14863.193, 24051.262],\n",
      "       [20327.494, 14863.087, 24052.092],\n",
      "       [20327.06 , 14862.979, 24052.89 ],\n",
      "       [20326.596, 14862.866, 24053.658],\n",
      "       [20326.102, 14862.752, 24054.395],\n",
      "       [20325.576, 14862.634, 24055.102],\n",
      "       [20325.021, 14862.514, 24055.775],\n",
      "       [20324.434, 14862.391, 24056.418],\n",
      "       [20323.816, 14862.264, 24057.03 ],\n",
      "       [20323.19 , 14862.177, 24057.615],\n",
      "       [20322.568, 14862.17 , 24058.18 ],\n",
      "       [20321.957, 14862.243, 24058.725],\n",
      "       [20321.354, 14862.398, 24059.248],\n",
      "       [20320.758, 14862.633, 24059.75 ],\n",
      "       [20320.17 , 14862.949, 24060.232],\n",
      "       [20319.592, 14863.346, 24060.691],\n",
      "       [20319.021, 14863.801, 24061.16 ],\n",
      "       [20318.459, 14864.295, 24061.664],\n",
      "       [20317.904, 14864.827, 24062.203],\n",
      "       [20317.36 , 14865.397, 24062.777],\n",
      "       [20316.822, 14866.006, 24063.389],\n",
      "       [20316.293, 14866.653, 24064.033],\n",
      "       [20315.771, 14867.338, 24064.715],\n",
      "       [20315.264, 14868.028, 24065.434],\n",
      "       [20314.77 , 14868.69 , 24066.191],\n",
      "       [20314.291, 14869.324, 24066.986],\n",
      "       [20313.828, 14869.931, 24067.822],\n",
      "       [20313.38 , 14870.509, 24068.695],\n",
      "       [20312.947, 14871.06 , 24069.607],\n",
      "       [20312.531, 14871.582, 24070.559],\n",
      "       [20312.154, 14872.077, 24071.549],\n",
      "       [20311.84 , 14872.544, 24072.576],\n",
      "       [20311.59 , 14872.983, 24073.645],\n",
      "       [20311.402, 14873.394, 24074.75 ],\n",
      "       [20311.28 , 14873.777, 24075.896],\n",
      "       [20311.219, 14874.133, 24077.08 ]], dtype=float32), 'swc_id': '8617461762'}\n"
     ]
    }
   ],
   "source": [
    "edge = util.sample_once(input_graph.edges)\n",
    "print(\"Edge attributes:\", input_graph.edges[edge])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b49f05f0-2dd0-468c-aeed-a6cc28c02309",
   "metadata": {},
   "source": [
    "Here is a visualization of the graph where each connected component is depicted in a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8da1a07-7745-49cc-affb-083eef50d567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if plot_bool:\n",
    "    viz.visualize_graph(input_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b602de-c0e3-44bc-89f8-70238f6b5234",
   "metadata": {},
   "source": [
    "### Proposal Generation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "632fa902-b4d0-4781-b9cd-5ad7c7e88df8",
   "metadata": {},
   "source": [
    "Our objective is to reconnect fragments in order to reconstruct the projections of individual neurons on a whole-brain scale. We generate \"edge proposals\", or just proposals, by iterating over all leaf nodes and connecting them to other leaf nodes within 30um. In this example, the proposals have already been generated and are stored as a graph-level attribute called \"proposals\". This attribute is a set containing pairs of nodes stored as a frozenset.\n",
    "\n",
    "Note: This repo supports the option to generate proposals between leaf nodes and a point in the middle of an edge. We refer to this type of proposal as a \"complex\" and a proposal between two leaf nodes as \"simple\". I've found that the models tend to have significantly lower precision for complex precisions. I typically only generate simple proposals during training and inference since precision is extremely important in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9825c712-04bd-4f89-9291-7718eb859427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a proposal: frozenset({835, 294})\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a proposal:\", util.sample_once(input_graph.proposals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e208e9-2e5b-4ecd-b6e1-53d5aa168315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Proposals: 450\n"
     ]
    }
   ],
   "source": [
    "print(\"# Proposals:\", len(input_graph.proposals))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cd9d119-c2e3-40d2-8a2c-bb540be6a2cf",
   "metadata": {},
   "source": [
    "Here is a visualization of the input graph shown in black and proposals shown in orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03781b67-b099-4627-96e0-049d5076069c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if plot_bool:\n",
    "    viz.visualize_proposals(input_graph, color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349321c-4ed1-4948-8e4e-07f5930a94a3",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a139c61-6b88-4202-9030-e803768520d4",
   "metadata": {},
   "source": [
    "A human annotator manually traced every individual neuron in the image chunk that the input graph is contained within. There is a module within this repo that uses these tracings to algorithmically determine which proposals should be accepted. One important detail is that this algorithm is almost always correct (approx 95-99%), but there a few mistakes. There are some cases where the fragments are not well aligned to the tracings due image artifacts. As you look closer at these tracings and fragments, you'll see that determining the groundtruth isn't always straightforward.\n",
    "\n",
    "The groundtruth for the proposals as the list of proposals that should be accepted. In the cells below, we show you how to access the ground truth accepts and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93c34c2-74a9-4120-8d79-cf3da5e6c71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a proposal that should be accepted: frozenset({213, 685})\n"
     ]
    }
   ],
   "source": [
    "p = util.sample_once(input_graph.gt_accepts)\n",
    "print(\"This is a proposal that should be accepted:\", p)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2034f30-b6cc-4b41-8fe6-61089246c3ca",
   "metadata": {},
   "source": [
    "Next, let's visualize the ground truth accepts with the ground truth tracings. In the plot below: fragments are black, proposals are orange, and ground truth tracings are colored such that each connected component is a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a77b89-dc36-44c3-8a71-9bd2a474b748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groundtruth_filename = f\"groundtruth_graph_{brain_id}-{example_id}.pkl\"\n",
    "path = f\"./groundtruth_graphs/{groundtruth_filename}\"\n",
    "with open(path, \"rb\") as file:\n",
    "    groundtruth_graph = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce4ddc1-9eee-4410-b8ae-889a6f115204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if plot_bool:\n",
    "    viz.visualize_subset(\n",
    "        input_graph,\n",
    "        input_graph.gt_accepts,\n",
    "        proposal_subset=True,\n",
    "        color=\"orange\",\n",
    "        groundtruth_graph=groundtruth_graph\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ed870-7b1b-4e7a-b4b9-1c8584224963",
   "metadata": {},
   "source": [
    "## Part 2: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7e043-8584-47e0-b0f8-fe6b2d5ec5bd",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ee34371-5492-4d92-86e1-dcf4f4108ac1",
   "metadata": {},
   "source": [
    "Next we'll load precomputed features for this graph.\n",
    "\n",
    "Note that we use the term \"branch\" to refer to edges from the input graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e8dcacc-9884-445e-b613-4fe9da198191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'dict'>\n",
      "features.keys(): dict_keys(['nodes', 'branches', 'proposals'])\n"
     ]
    }
   ],
   "source": [
    "features_filename = f\"input_graph_features_{brain_id}-{example_id}.pkl\"\n",
    "with open(f\"./features/{features_filename}\", \"rb\") as file:\n",
    "    features = pickle.load(file)\n",
    "print(\"type(features):\", type(features))\n",
    "print(\"features.keys():\", features.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a1b992b-9f9b-4de0-86b9-60a725bd4a8c",
   "metadata": {},
   "source": [
    "Here's an example that gives you an idea of how the features are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa24b56-69d7-4f1c-949e-b1c1de930c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features: [1.         1.24880099 1.        ]\n",
      "Branch features: [2.20507812 0.10569328]\n",
      "Proposal features: [ 0.26029137  1.          2.45673299  2.87637806  0.58817631  0.57754725\n",
      " -0.92830652  0.65433437  0.52527463 -0.91036189  0.76496297  0.56087697\n",
      " -0.94616777  0.817469    0.62327921 -0.94112843  0.19617225  0.23444976\n",
      "  0.23444976  0.37799043  0.26315789  0.26315789  0.33014354  0.31578947\n",
      "  0.31578947  0.36842105  0.36842105  0.42105263  0.45454545  0.45454545\n",
      "  0.44019139  0.46889952  0.34419856  0.08598255]\n"
     ]
    }
   ],
   "source": [
    "# Node features\n",
    "node = util.sample_once(input_graph.nodes)\n",
    "print(\"Node features:\", features[\"nodes\"][node])\n",
    "\n",
    "# Branch features\n",
    "branch = util.sample_once(input_graph.edges)\n",
    "print(\"Branch features:\", features[\"branches\"][frozenset(branch)])\n",
    "\n",
    "# Proposal features\n",
    "proposal = util.sample_once(input_graph.proposals)\n",
    "print(\"Proposal features:\", features[\"proposals\"][proposal])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51176370-29c1-45bc-b04f-99055f254527",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcf11130-0b00-4996-b798-47d6d3768fed",
   "metadata": {},
   "source": [
    "Next, we'll initialize an instance of a custom heterogeneous graph dataset, which organizes the feature vectors of nodes, branches, and proposals into individual matrices. This class has an \"index mappings\" attributes that map node, branch, or proposal IDs to their respective indices in the feature matrix. \n",
    "\n",
    "The computation graph for our GNN combines the input graph with all proposals added as edges, making it heterogeneous, with edges labeled as either 'branch' (original edges) or 'proposal'. We'll convert this computation graph into a line graph so that we can more easily apply message passing modules from PyTorch_Geometric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee123b31-6cdb-4a44-814e-50783c0c7ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ml_util.init_dataset(\n",
    "    input_graph,\n",
    "    features,\n",
    "    computation_graph=input_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e59d76-6fa5-47a0-9dd8-fc29cfe0b479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Attributes: dict_keys(['idxs_branches', 'idxs_proposals', 'computation_graph', 'proposals', 'node_types', 'edge_types', 'data', 'n_edge_attrs'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Attributes:\", dataset.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635617d2-ca8a-4735-99b9-65cc7d677127",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index mappings for proposals: dict_keys(['idx_to_id', 'id_to_idx'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Index mappings for proposals:\", dataset.idxs_proposals.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3982a7f9-fccb-4b08-a558-79dcda790e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature vector of proposal 'frozenset({104, 334})' is in the 261-th row of the feature matrix dataset.data['proposal'].x\n"
     ]
    }
   ],
   "source": [
    "proposal = util.sample_once(input_graph.proposals)\n",
    "idx = dataset.idxs_proposals[\"id_to_idx\"][proposal]\n",
    "print(f\"The feature vector of proposal '{proposal}' is in the {idx}-th row of the feature matrix dataset.data['proposal'].x\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d04b44e-fd56-4ab4-9fd6-91f9cc29ccdf",
   "metadata": {},
   "source": [
    "Here is a sanity check that the idx mapping is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f39a092f-43d3-4457-87a2-2801eb23a6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.20757942e-09  9.65816004e-10 -3.31136915e-09 -1.37973715e-09\n",
      " -1.24176344e-09  1.10378972e-09  1.51771086e-09  1.10378972e-09\n",
      " -8.27842288e-10  1.51771086e-09 -1.24176344e-09 -8.27842288e-10\n",
      "  1.51771086e-09 -1.79365829e-09  0.00000000e+00 -1.37973715e-09\n",
      " -8.27842295e-10  2.17256293e-10]\n"
     ]
    }
   ],
   "source": [
    "print(features[\"proposals\"][proposal] - np.array(dataset.data[\"proposal\"].x[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2b35b-629b-4a07-8ea9-862ce81a3e8d",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73cb3fe2-620f-48a0-9767-da414a7f3565",
   "metadata": {},
   "source": [
    "Next let's load a GNN model and use it to perform inference on the proposals. We'll then create a dictionary that maps proposals to  \"edge beliefs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2938dba8-3d9d-41e4-82b0-0e973fc16d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = \"./GraphNeuralNet-2024-10-12.pth\"\n",
    "model = ml_util.load_model(model_path)\n",
    "\n",
    "# Generate proposal prediction\n",
    "preds_vector = inference.predict_with_gnn(model, dataset.data, device=\"cpu\")\n",
    "preds_dict = {dataset.idxs_proposals[\"idx_to_id\"][i]: p for i, p in enumerate(preds_vector)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83d8f7fe-58bc-4a0c-95b3-f82284ba6445",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge belief of proposal 'frozenset({454, 22})' is 0.16676826775074005\n"
     ]
    }
   ],
   "source": [
    "proposal = util.sample_once(input_graph.proposals)\n",
    "print(f\"Edge belief of proposal '{proposal}' is {preds_dict[proposal]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40d8d6-d05c-466e-992f-582bdaec018c",
   "metadata": {},
   "source": [
    "### Accepted Proposals"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39938a1c-816a-4d3b-9e81-64bbe8d57fb7",
   "metadata": {},
   "source": [
    "Lastly, we need to use the edge beliefs to determine which proposals to accept and add to the input graph. The proposals are first filtered by removing any proposal with an edge belief below some threshold (i.e. accePtance_threshold). Next we iterate over the remaining proposals and check that adding this proposal to the input graph (with previously accepted proposals) does not create a cycle. A proposal that satisfies this criteria is said to be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "150a245f-a247-405e-9903-30ae3a710596",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Proposals Accepted: 0.4022222222222222\n"
     ]
    }
   ],
   "source": [
    "acceptance_threshold = 0.75\n",
    "accepts = inference.get_accepts(input_graph, preds_dict, acceptance_threshold)\n",
    "\n",
    "print(\"% Proposals Accepted:\", len(accepts) / input_graph.n_proposals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7ceac-410c-4d7c-8dec-6c099f7470c6",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "655098ea-3f15-407c-9e75-98ef8c3a7b78",
   "metadata": {},
   "source": [
    "First, let's visualize the beliefs of proposals that are in our \"input_graph.gt_accepts\" versus those which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6c4ef-4b17-4cfa-b633-ee1973768980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gt_accept_beliefs = np.array([preds_dict[p] for p in input_graph.gt_accepts])\n",
    "gt_reject_beliefs = np.array([preds_dict[p] for p in input_graph.proposals if p not in input_graph.gt_accepts])\n",
    "\n",
    "print(\"Median belief of proposals in gt accepts:\", np.median(gt_accept_beliefs))\n",
    "print(\"Median belief of proposals not in gt accepts:\", np.median(gt_reject_beliefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a56922-db8c-4916-9c7c-8b7341f54374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 4.5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(gt_accept_beliefs, bins=30, color='tab:green', alpha=0.7, label='Proposals in gt accepts')\n",
    "plt.hist(gt_reject_beliefs, bins=30, color='tab:red', alpha=0.7, label='Proposals not in gt accepts')\n",
    "plt.xlabel('Belief', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Proposal Beliefs', fontsize=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cf09c98-32b1-4fb1-8bfd-5dd755e7b972",
   "metadata": {},
   "source": [
    "There is a built-in module that can be used to compute the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e6bc2-3b6f-41f4-a2a5-6652692ae870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = evaluator.run_evaluation(input_graph, input_graph.list_proposals(), accepts)\n",
    "\n",
    "# Report results\n",
    "results = results[\"Overall\"]\n",
    "for metric in results:\n",
    "    print(f\"   {metric} = {np.round(results[metric], 4)}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
